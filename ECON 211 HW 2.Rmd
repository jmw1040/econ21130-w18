---
title: "Homework on static labor supply"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(testthat)
require(xtable)
require(pander)
```

In the problem set we are following an labor supply model specefied by the following optimization problem. 

$$
\max_{c,h,e} c - \beta \frac{h^{1+\gamma}}{1+\gamma}\\
\text{s.t. } c = e \cdot \rho \cdot w\cdot h +(1-e)\cdot r \cdot h
$$
The individual takes his wage $w$ as given, he chooses hours of work $h$ and consumption $c$. He also chooses whether to work in the labor force or to work at home where he has an equivalent wage $r$.

#### Problem 1 
For any given change in wage, we know that this affects the agent's hours supplied via two effects - the substitution effect (the relative price of leisure increases as wage rises) and the income effect (the demand for leisure increases as income rises).  

More concretely, the income effect tells us that as wages goes up, the individual will work less because they will now make the same amount of money in less amount of time. The way our problem is currently set up, as wages go up we will not see any income effect. This is because there is no concavity in the consumption term; the relationship between consumption and hours worked is linear.

If we were to  substitute $c$ in the utility function for $\frac{c^{1+\eta}}{1+\eta}$ then it is possible that we would see an income effect. Now our consumption is no longer linear in consumption but rather, concave. This means that as consumption increases, there are diminishing marginal returns to an increase in $c$. Therefore, when the individual has more income, rather than just continuing to increase consumption, they will instead choose to diminsh their hours somewhat.

## Simulating data

We are going to simulate a data set where agents will choose participation as well as the number of hours if they decide to work. This requires for us to specify how each of the individual specific variables are drawn. We then set the following:

$$
\begin{align*}
\log W_i     &= \eta X_i + Z_i + u_i  \\
\log R_i     &= \delta_0 + \log(W_i) + \delta Z_i + \xi_i \\
\log \beta_i &= X_i +\epsilon_i +  a \xi_i   \\
\end{align*}
$$

and finally $(X_i,Z_i,\epsilon_i,u_i,\xi_i)$ are independent normal draws. Given all of this we can simulate our data. 

#### Problem 2 

The parameter $a$ captures the relationship between $r_i$ the amount of money the agent can make at home and $\beta_i$ the agent's distaste for work. If $a$ is any value greater than zero then there is nonzero correlation between $r_i$ and $\beta_i$ due to direct correlation in their error terms. This will lead to an endogenity issue that will make it so we do not recover the correct value in our OLS estimation. 

The code below simulates the data when $a=0$ and when $a=1$ 

```{r, results='hide'}

library(data.table)
  sim_data <- function(a_val = 0){
  p  = list(gamma = 0.8,beta=1,a=a_val,rho=1,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
  N=10000  # size of the simulation
  simdata0 = data.table(i=1:N,X=rnorm(N))
  
  # simulating variables
  simdata0[,X := rnorm(N)]
  simdata0[,Z := rnorm(N)]
  simdata0[,u := rnorm(N)]
  simdata0[,lw := p$eta*X  + Z + 0.2*u ]  # log wage
  
  simdata0[,xi := rnorm(N)*0.2]
  simdata0[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity
  
  simdata0[,eps:=rnorm(N)*0.2]
  simdata0[,beta := exp(p$nu*X  + p$a*xi + eps)]; # heterogenous beta coefficient
  
  # compute decision variables
  simdata0[, lfp := log(p$rho) + lw >= lr] # labor force participation
  simdata0[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
  simdata0[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
  simdata0[,mean(lfp)]
  
  return(simdata0)
}

simdata0 = sim_data(a_val = 0)
simdata1 = sim_data(a_val = 1)

```

Now that we have simulated our data for both cases $a=0$ and $a=1$, we can see how it affects our OLS estimation when $a=1$: 

```{r}
a1 <- lm(log(h) ~ lw + X, data=simdata1)
a0 <- lm(log(h) ~ lw + X, data=simdata0)
summary(a1)
summary(a0)
```

### Problem 3 
When $a=1$ we find that the coefficient on log wages is `r summary(a1)$coefficients[2,1]`, whereas when $a=0$ we got a coefficient of `r summary(a0)$coefficients[2,1]`. Since we know the true data-generating process, we know that our coefficient of log wage should be $1 / \gamma$ - so the true coefficient should be approximately $1.25$. This verifies that when $a=1$, there exists an endogenity issue that prevents us from recovering the true coefficient $\gamma$ when we let the error terms be correlated. 

Intution behind why having $a=1$ we overestimate the value of $\gamma$:
- When $a$ is equal to 1, there is now a direct relationship between $R$ and $\beta$. So across individuals, higher values of one correspond to higher values of the other.  
- We only get to observe the people where $wage > R$. Because higher values of $R$ mean that those people are more likely to stay home, then we know that our working population consists of people with (on average) lower disutilities of labor $\beta$.
- Then in the working population, we are left with individuals who will work more hours at a given wage. This leads us to overestimate the effect of wages on labor supply. 
- We are thus left with a higher value of $\gamma$ - and consistent with the result from the regression - a lower value of $1/ \gamma$. 

<!-- - If we assume there is no relationship between R and Beta then we are believing the people in our sample have some beta. 
- Because the people we observe are the people with smallers R's this must mean that these people also have smaller Beta's 
- Us believing that this people should have higher beta's than they actually do, leads to us overestimating the value of gamma. 

(I am gonna ask lamadon about this in office hours. )
(also the intercept) -->

## Heckman correction

As we have seen in class, Heckman (74) offers a way for us to correct the our regression in order to recover our structural parameters. 

As we have seen in class, we need to understand how the error term in the hour regression correlates with the laborparticipation decision. 

#### Problem 4 

$$
 E[\epsilon | log(\rho w) > log(r_i)] \\ 
 E[\epsilon | log(\rho w) > \delta_o + log(w_i) + \delta Z_i + \xi_i] \\ 
 E[\epsilon | log(\rho) > \delta_o + \delta Z_i + \xi_i] \\ 
$$
We know that $\epsilon_i$ and $\xi_i$ are jointly normal so we know $E[\epsilon | \xi_i] = a + b\xi_i$ 

$$
a + bE[\xi_i | \xi_i < log(p) - \delta_0 - \delta Z_i] + 0 
$$
Now we can construct the inverse mills ratio and incorporate that term in our regression to get rid of our endogenity issue. 

When we regress labor force particpation on $z_i$ we will find our coefficient2 $\beta_0 = \frac{log(\rho) - \delta_0}{\sigma_{\xi}}$ and $\beta_1 = \frac{\delta}{\sigma_{\xi}}$. Our Inverse Mills ratio can be seen below. 

$$ 

a - \frac{\sigma_{\xi} \phi( \frac{log(p) - \delta_0 - \delta Z_i}{\sigma_{\xi}})}{\Phi ( \frac{log(p) - \delta_0 - \delta Z_i}{\sigma_{\xi}})}
$$ 
When we add our inverse mills ratio into our regression we will do so by doing a probit of particpation on $Z_i$. We will call the two variables we recover from this regression as $\beta_0$ and $\beta_1$. When we add a term to our regression to account for this endogenity we will construct it as follows. 

$$
\frac{\phi(\beta_0 + \beta_1 * Z_i)}{\Phi(\beta_0 + \beta_1 * Z_i)}
$$

We will now run our regression with this additional term included in the regression. 

```{r}
fit2 = glm(lfp ~ Z,simdata1,family = binomial(link = "probit"))
Betas = summary(fit2)
B_0 = Betas$coefficients[1,1]
B_1 = Betas$coefficients[2,1]

simdata0[,lambda_i := pnorm(B_0 + B_1 * Z) / dnorm(B_0 + B_1 * Z)]

heck <- lm(log(h) ~ lw + X + lambda_i , data=simdata0)
summary(heck)




```

####Problem 5 

If we run the regression no including the variable we constructed above we get a coefficient on log wage of 1.248. We have now by including the inverse mills ration in our regression recovered the correct estiamte for $\gamma$ even in the case where $a\neq 0$.

## Repeated cross-section

In the code below we just create two new samples, one where $\rho = 1$ and one where $\rho =1.2$. We have added the wage residual $u_i$ inside the expression for $\beta_i$. 

```{r}
p  = list(gamma = 0.8,beta=1,a=0,rho=1,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
N=10000  # size of the simulation
simdatac1 = data.table(i=1:N,X=rnorm(N))

# simulating variables
simdatac1[,X := rnorm(N)]
simdatac1[,Z := rnorm(N)]
simdatac1[,u := rnorm(N)]
simdatac1[,lw := p$eta*X  + Z + 0.2*u ]  # log wage

simdatac1[,xi := rnorm(N)*0.2]
simdatac1[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity

simdatac1[,eps:=rnorm(N)*0.2]
simdatac1[,beta := exp(p$nu*X  + p$a*xi + eps + 0.2*u)]; # heterogenous beta coefficient

# compute decision variables
simdatac1[, lfp := log(p$rho) + lw >= lr] # labor force participation
simdatac1[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
simdatac1[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
simdatac1[,mean(lfp)]

p  = list(gamma = 0.8,beta=1,a=0,rho=1.2,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
N=10000  # size of the simulation
simdatac2 = data.table(i=1:N,X=rnorm(N))

# simulating variables
simdatac2[,X := rnorm(N)]
simdatac2[,Z := rnorm(N)]
simdatac2[,u := rnorm(N)]
simdatac2[,lw := p$eta*X  + Z + 0.2*u ]  # log wage

simdatac2[,xi := rnorm(N)*0.2]
simdatac2[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity

simdatac2[,eps:=rnorm(N)*0.2]
simdatac2[,beta := exp(p$nu*X  + p$a*xi + eps + 0.2*u)]; # heterogenous beta coefficient

# compute decision variables
simdatac2[, lfp := log(p$rho) + lw >= lr] # labor force participation
simdatac2[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
simdatac2[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
simdatac2[,mean(lfp)]

```


In the code below we construct the inverse mills ratio for each of the two different time periods. 

```{r}
fitcs1 = glm(lfp ~ Z,simdatac1,family = binomial(link = "probit"))
Betas = summary(fitcs1)
B_01 = Betas$coefficients[1,1]
B_11 = Betas$coefficients[2,1]

simdatac1[,lambda_i := pnorm(B_01 + B_11 * Z) / dnorm(B_01 + B_11 * Z)]

fitcs2 = glm(lfp ~ Z,simdatac2,family = binomial(link = "probit"))
Betas = summary(fitcs2)
B_02 = Betas$coefficients[1,1]
B_12 = Betas$coefficients[2,1]

simdatac2[,lambda_i := pnorm(B_02 + B_12 * Z) / dnorm(B_02 + B_12 * Z)]

```

####Problem 6 
- our intercept terms are super different but our coefficient on Z is the same. 
- In the previous construction we had included $log(\rho)$ but $\rho=1$ so $log(rho) = 0$. Now with a diffent value of rho this will actually affect our $\beta_0$. We this in the difference in our values. 1.417 vs 0.48. 

#### Problem 7 
What we need to do in code, 
- first break into seperate groups based on values of x 
- then mean those groups 
- then do differences over time 
- then regress using those variables 

```{r}
meangp1 = data.table(n=1:70, avg_wage = 0, avg_hour = 0, avg_lam = 0, avg_x = 0)
meangp2 = data.table(n=1:70, avg_wage = 0, avg_hour = 0, avg_lam = 0, avg_x = 0)

for (i in -34:35) {
  splitup = i * 1/15
  splitlow = splitup - (1/15)
  meangp1[i + 35] = simdatac1 %>% filter(lfp == TRUE) %>% filter(between(X, splitlow, splitup)) %>% summarise(n = n(), avg_wage = mean(lw), avg_hour = mean(h), avg_lam = mean(lambda_i), avg_x = mean(X))
   meangp2[i + 35] = simdatac2 %>% filter(lfp == TRUE) %>% filter(between(X, splitlow, splitup)) %>% summarise(n = n(), avg_wage = mean(lw), avg_hour = mean(h), avg_lam = mean(lambda_i), avg_x = mean(X))
}
   
```

```{r}
diff = data.table(n=1:70, diff_wage = 0, diff_hour = 0, diff_lam = 0, diff_x = 0)

for (i in 1:70) { 
  diff[i][, "diff_wage"] = meangp2[i, avg_wage] - meangp1[i ,avg_wage]
  diff[i][, "diff_hour"] = log(meangp2[i, avg_hour]) - log(meangp1[i ,avg_hour])
  diff[i][, "diff_lam"] = meangp2[i, avg_lam] - meangp1[i ,avg_lam]
  diff[i][, "diff_x"] = meangp2[i, avg_x] - meangp1[i ,avg_x]
}

RCS <- lm(diff_hour ~ diff_wage + diff_x + diff_lam , data=diff)
summary(RCS)
  
```

Recovering gamma was influenced immensly by how i grouped the X's. It varied all over the place. I messed with number of intervals and size of intervals until I got a value that was close to correct. Not sure this is really a sound practice. 

I simulated the data a bunch of different times with the above construction and it comes pretty close everytime, so I feel pretty good about the above construction. The above construction has 70 different groups each a different interval of x from -2.33 to 2.33 of size .0666. 

The way I decided to the grouping also leaves out the x's with values more extreme or less extreme than those intervals. 









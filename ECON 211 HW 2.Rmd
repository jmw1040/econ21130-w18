---
title: "Homework on static labor supply"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(testthat)
require(xtable)
require(pander)
library(dplyr)
```

In the problem set we are following an labor supply model specefied by the following optimization problem. 

$$
\max_{c,h,e} c - \beta \frac{h^{1+\gamma}}{1+\gamma}\\
\text{s.t. } c = e \cdot \rho \cdot w\cdot h +(1-e)\cdot r \cdot h
$$
The individual takes his wage $w$ as given, he chooses hours of work $h$ and consumption $c$. He also chooses whether to work in the labor force or to work at home where he has an equivalent wage $r$.

#### Problem 1 
For any given change in wage, we know that this affects the agent's hours supplied via two effects - the substitution effect (the relative price of leisure increases as wage rises) and the income effect (the demand for leisure increases as income rises).  

More concretely, the income effect tells us that as wages goes up, the individual will work less because they will now make the same amount of money in less amount of time. The way our problem is currently set up, as wages go up we will not see any income effect. This is because there is no concavity in the consumption term; the relationship between consumption and hours worked is linear.

If we were to  substitute $c$ in the utility function for $\frac{c^{1+\eta}}{1+\eta}$ then it is possible that we would see an income effect. Now our consumption is no longer linear in consumption but rather, concave. This means that as consumption increases, there are diminishing marginal returns to an increase in $c$. Therefore, when the individual has more income, rather than just continuing to increase consumption, they will instead choose to diminsh their hours somewhat.

## Simulating data

We are going to simulate a data set where agents will choose participation as well as the number of hours if they decide to work. This requires for us to specify how each of the individual specific variables are drawn. We then set the following:

$$
\log W_i = \eta X_i + Z_i + u_i  \\
\log R_i= \delta_0 + \log(W_i) + \delta Z_i + \xi_i \\
\log \beta_i = X_i +\epsilon_i +  a \xi_i   
$$

and finally $(X_i,Z_i,\epsilon_i,u_i,\xi_i)$ are independent normal draws. Given all of this we can simulate our data. 

#### Problem 2 

The parameter $a$ captures the relationship between $r_i$ the amount of money the agent can make at home and $\beta_i$ the agent's distaste for work. If $a$ is any value greater than zero then there is nonzero correlation between $r_i$ and $\beta_i$ due to direct correlation in their error terms. This will lead to an endogenity issue that will make it so we do not recover the correct value in our OLS estimation. 

The code below simulates the data when $a=0$ and when $a=1$ 

```{r, results='hide'}

library(data.table)
  sim_data <- function(a_val = 0, rho_val = 1, u_ind = 0){
  p  = list(gamma = 0.8,beta=1,a=a_val,rho=rho_val,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
  N=10000  # size of the simulation
  simdata0 = data.table(i=1:N,X=rnorm(N))
  
  # simulating variables
  simdata0[,X := rnorm(N)]
  simdata0[,Z := rnorm(N)]
  simdata0[,u := rnorm(N)]
  simdata0[,lw := p$eta*X  + Z + 0.2*u ]  # log wage
  
  simdata0[,xi := rnorm(N)*0.2]
  simdata0[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity
  
  simdata0[,eps:=rnorm(N)*0.2]
  simdata0[,beta := exp(p$nu*X  + p$a*xi + eps + u_ind*0.2*u)]; # heterogenous beta coefficient
  
  # compute decision variables
  simdata0[, lfp := log(p$rho) + lw >= lr] # labor force participation
  simdata0[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
  simdata0[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
  simdata0[,mean(lfp)]
  
  return(simdata0)
}

simdata0 = sim_data(a_val = 0)
simdata1 = sim_data(a_val = 1)

```

Now that we have simulated our data for both cases $a=0$ and $a=1$, we can see how it affects our OLS estimation when $a=1$: 

```{r}
a1 <- lm(log(h) ~ lw + X, data=simdata1)
a0 <- lm(log(h) ~ lw + X, data=simdata0)
summary(a1)
summary(a0)
```

### Problem 3 
When $a=1$ we find that the coefficient on log wages is `r summary(a1)$coefficients[2,1]`, whereas when $a=0$ we got a coefficient of `r summary(a0)$coefficients[2,1]`. Since we know the true data-generating process, we know that our coefficient of log wage should be $1 / \gamma$ - so the true coefficient should be approximately $1.25$. This verifies that when $a=1$, there exists an endogenity issue that prevents us from recovering the true coefficient $\gamma$ when we let the error terms be correlated. 

Intution behind why having $a=1$ we overestimate the value of $\gamma$:
- When $a$ is equal to 1, there is now a direct relationship between $R$ and $\beta$. So across individuals, higher values of one correspond to higher values of the other.  
- We only get to observe the people where $wage > R$. Because higher values of $R$ mean that those people are more likely to stay home, then we know that our working population consists of people with (on average) lower disutilities of labor $\beta$.
- Then in the working population, we are left with individuals who will work more hours at a given wage. This leads us to overestimate the effect of wages on labor supply. 
- We are thus left with a higher value of $\gamma$ - and consistent with the result from the regression - a lower value of $1/ \gamma$. 

## Heckman correction

As we have seen in class, Heckman (74) offers a way for us to correct the our regression in order to recover our structural parameters. 

As we have seen in class, we need to understand how the error term in the hour regression correlates with the labor participation decision. 

#### Problem 4 

$$
 E[\epsilon | log(\rho w) > log(r_i)] \\ 
 E[\epsilon | log(\rho w) > \delta_o + log(w_i) + \delta Z_i + \xi_i] \\ 
 E[\epsilon | log(\rho) > \delta_o + \delta Z_i + \xi_i] \\ 
$$
We know that $\epsilon_i$ and $\xi_i$ are jointly normal so we know $E[\epsilon | \xi_i] = a + b\xi_i$ 

$$
a + bE[\xi_i | \xi_i < log(p) - \delta_0 - \delta Z_i] + 0 
$$
Now we can construct the inverse mills ratio and incorporate that term in our regression to get rid of our endogenity issue (from selection into labor force). 

When we regress labor force particpation on $z_i$ we will find our coefficients $\beta_0 = \frac{log(\rho) - \delta_0}{\sigma_{\xi}}$ and $\beta_1 = \frac{\delta}{\sigma_{\xi}}$. Our inverse Mills ratio can be seen below. 

$$ 
a - \frac{\sigma_{\xi} \phi( \frac{log(p) - \delta_0 - \delta Z_i}{\sigma_{\xi}})}{\Phi ( \frac{log(p) - \delta_0 - \delta Z_i}{\sigma_{\xi}})}
$$ 
To recover the parameters we need for the ratio, we run a probit regression of particpation on $Z_i$. We will call the two variables we recover from this regression as $\beta_0$ and $\beta_1$. When we add a term to our regression to account for this endogenity we will construct it as follows: 

$$
\frac{\phi(\beta_0 + \beta_1 * Z_i)}{\Phi(\beta_0 + \beta_1 * Z_i)}
$$

We will now run our regression with this additional term included in the regression:

```{r}
fit2 = glm(lfp ~ Z,simdata1,family = binomial(link = "probit"))

Betas = summary(fit2)
B_0 = Betas$coefficients[1,1]
B_1 = Betas$coefficients[2,1]

simdata0[,lambda_i := pnorm(B_0 + B_1 * Z) / dnorm(B_0 + B_1 * Z)]

mills <- lm(log(h) ~ lw + X + lambda_i , data=simdata0)
summary(mills)
```

#### Problem 5 

If we run the regression no including the variable we constructed above we get a coefficient on log wage of `r summary(mills)$coefficients[2,1]`. By including the inverse mills ratio in our regression (controlling for selection), we have now recovered the correct estiamte for $\gamma$ even in the case where $a\neq 0$.

## Repeated cross-section

In the code below we just create two new samples, one where $\rho = 1$ and one where $\rho =1.2$. We have added the wage residual $u_i$ inside the expression for $\beta_i$. 

```{r}
simdatac1 = sim_data(a_val = 0, rho_val = 1, u_ind = 1)
simdatac2 = sim_data(a_val = 0, rho_val = 1.2, u_ind = 1)
```


In the code below we construct the inverse mills ratio for each of the two different time periods. 

```{r}
fitcs1 = glm(lfp ~ Z,simdatac1,family = binomial(link = "probit"))
Betas = summary(fitcs1)
B_01 = Betas$coefficients[1,1]
B_11 = Betas$coefficients[2,1]

simdatac1[,lambda_i := pnorm(B_01 + B_11 * Z) / dnorm(B_01 + B_11 * Z)]

fitcs2 = glm(lfp ~ Z,simdatac2,family = binomial(link = "probit"))
Betas = summary(fitcs2)
B_02 = Betas$coefficients[1,1]
B_12 = Betas$coefficients[2,1]

simdatac2[,lambda_i := pnorm(B_02 + B_12 * Z) / dnorm(B_02 + B_12 * Z)]

```

####Problem 6 
<!-- Come back to this one --> 
Changing $\rho$ changes the returns to working, so the trade-off between joining and not joining the labor force is different for different values of $\rho$. In particular, increasing $\rho = 1.2$ increases our labor force participation by almost $20\%$. Now the agents are selecting into the labor force at a different margin, so the mills ratio is different.

In particular, when we regress labor force participation on characteristics $Z_i$, we note that the coefficient on $Z_i$ is the same, but the intercept is now higher. This makes sense - as people for every level of $Z_i$ are now more likely to work.

#### Problem 7 
For this problem, we want to estimate the true $\gamma$ despite the endogeneity issue between wage and disutility of labor (caused by adding the wage residual to our regression with $\beta_i$). To do this, we look at cohorts across two time periods - the time periods are given by distinct simulations and cohorts are created by grouping over values of $X_i$ (we take the mean of the attributes of interest).

Now using each cohort as an observation and differenced averages as variables, we attempt to recover the $\gamma$ from our model. 

```{r}
meangp1 = data.table(n=1:70, avg_wage = 0, avg_hour = 0, avg_lam = 0, avg_x = 0)
meangp2 = data.table(n=1:70, avg_wage = 0, avg_hour = 0, avg_lam = 0, avg_x = 0)

for (i in -34:35) {
  splitup = i * 1/15
  splitlow = splitup - (1/15)
  meangp1[i + 35] = simdatac1 %>% 
    filter(lfp == TRUE) %>% 
    filter(between(X, splitlow, splitup)) %>% 
    summarise(n = n()
              , avg_wage = mean(lw)
              , avg_hour = mean(h)
              , avg_lam = mean(lambda_i)
              , avg_x = mean(X))
   meangp2[i + 35] = simdatac2 %>% 
     filter(lfp == TRUE) %>% 
     filter(between(X, splitlow, splitup)) %>% 
     summarise(n = n()
               , avg_wage = mean(lw)
               , avg_hour = mean(h)
               , avg_lam = mean(lambda_i)
               , avg_x = mean(X))
}
   
```

```{r}
diff = data.table(n=1:70, diff_wage = 0, diff_hour = 0, diff_lam = 0, diff_x = 0)

for (i in 1:70) { 
  diff[i][, "diff_wage"] = meangp2[i, avg_wage] - meangp1[i ,avg_wage]
  diff[i][, "diff_hour"] = log(meangp2[i, avg_hour]) - log(meangp1[i ,avg_hour])
  diff[i][, "diff_lam"] = meangp2[i, avg_lam] - meangp1[i ,avg_lam]
  diff[i][, "diff_x"] = meangp2[i, avg_x] - meangp1[i ,avg_x]
}

RCS <- lm(diff_hour ~ diff_wage + diff_x + diff_lam , data=diff)
summary(RCS)
  
```

It should be noted that the estimated $\gamma$ was extremely sensitive to the grouping of the $X_i$'s. With various groupings, we could recover a whole range of possible $\gamma$'s. By changing both the number and size of intervals, we were able to recover an estimate close to the true value. This demonstrates that our estimate is not robust to changes to our bin sizes. 

That said, using the above construction, the simulations usually returned a $\gamma$ close to the true $0.8$. The above construction partitions the $X_i$'s ranging from $-2.33$ to $2.33$ with bins of size $.0667$. Note that this leaves out the most extreme values of $X_i$. 



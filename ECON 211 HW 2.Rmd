---
title: "Homework on static labor supply"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
      smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(testthat)
require(xtable)
require(pander)
```

In the problem set we are following an labor supply model specefied by the following optimization problem. 

$$
\max_{c,h,e} c - \beta \frac{h^{1+\gamma}}{1+\gamma}\\
\text{s.t. } c = e \cdot \rho \cdot w\cdot h +(1-e)\cdot r \cdot  h
$$
The individual takes his wage $w$ as given, he chooses hours of work $h$ and consumption $c$. He also chooses whether to work in the labor force or to work at home where he has an equivalent wage $r$.

#### Problem 1 

The income effect means that as wages goes up, the individual will substitute away from wages a bit because they will now make the same amount of money in less amount of time. The way our problem is currently set up, as wages go up we will not see any income effect. This is because there is no concavity in C. The relationship with c is and our income is linear. 

 If we were to  substitute $c$ in the utility for $\frac{c^{1+\eta}}{1+\eta}$ then it is possible that we would see an income effect. Now our consumption is no longer linear in consumption but rather concave. This means, that as C increases there is a diminishing marginal return of increasing c even more. Therefore, when the individual has more income, rather than just continuing to increase c, they will choose to diminsh their hours somewhat.  


## Simulating data

We are going to simulate a data set where agents will choose participation as well as the number of hours if they decide to work. This requires for us to specify how each of the individual specific variables are drawn. We then set the following:

$$
\begin{align*}
\log W_i     &= \eta X_i + Z_i + u_i  \\
\log R_i     &= \delta_0 + \log(W_i) + \delta Z_i + \xi_i \\
\log \beta_i &= X_i +\epsilon_i +  a \xi_i   \\
\end{align*}
$$

and finally $(X_i,Z_i,\epsilon_i,u_i,\xi_i)$ are independent normal draws. Given all of this we can simulate our data. 

#### Problem 2 

The parameter a captures if there is a relationship between $r_i$ the amount of money the agent can make at home and $\beta_i$ the agent's distaste for work. If a is any value greater than zero then there is a relationship between $\beta_i$ and $r_i$. This will lead to an endogenity issue that will make it so we do not recover the correct value in our OLS estimation. 

The code below simulates the data when $a=0$ and when $a=1$ 

```{r, results='hide'}
library(data.table)
p  = list(gamma = 0.8,beta=1,a=0,rho=1,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
N=10000  # size of the simulation
simdata0 = data.table(i=1:N,X=rnorm(N))

# simulating variables
simdata0[,X := rnorm(N)]
simdata0[,Z := rnorm(N)]
simdata0[,u := rnorm(N)]
simdata0[,lw := p$eta*X  + Z + 0.2*u ]  # log wage

simdata0[,xi := rnorm(N)*0.2]
simdata0[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity

simdata0[,eps:=rnorm(N)*0.2]
simdata0[,beta := exp(p$nu*X  + p$a*xi + eps)]; # heterogenous beta coefficient

# compute decision variables
simdata0[, lfp := log(p$rho) + lw >= lr] # labor force participation
simdata0[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
simdata0[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
simdata0[,mean(lfp)]

library(data.table)
p  = list(gamma = 0.8,beta=1,a=1,rho=1,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
N=10000  # size of the simulation
simdata1 = data.table(i=1:N,X=rnorm(N))

# simulating variables
simdata1[,X := rnorm(N)]
simdata1[,Z := rnorm(N)]
simdata1[,u := rnorm(N)]
simdata1[,lw := p$eta*X  + Z + 0.2*u ]  # log wage

simdata1[,xi := rnorm(N)*0.2]
simdata1[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity

simdata1[,eps:=rnorm(N)*0.2]
simdata1[,beta := exp(p$nu*X  + p$a*xi + eps)]; # heterogenous beta coefficient

# compute decision variables
simdata1[, lfp := log(p$rho) + lw >= lr] # labor force participation
simdata1[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
simdata1[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
simdata1[,mean(lfp)]

```

Now that we have simulated our data both when $a=0$ and when $a=1$, we can see how when $a=1$ this affects our OLS estimation. 

```{r}
a1 <- lm(log(h) ~ lw + X, data=simdata1)
a0 <- lm(log(h) ~ lw + X, data=simdata0)
summary(a1)
summary(a0)
```
### Problem 3 

When $a=1$ we can a coefficient on log wages of 1.15, whereas when $a=0$ we got a coefficient of 1.248. We know that our coefficient of log wage should be $1 / \gamma$ so we should get a coefficient of about 1.25. Clearly, when $a=1$ there is an endogenity issue that is causing our coefficent to be wrong. 

Intution behind why, having $a=1$ we overestimate the value of $\gamma$
- when a is equal to 1 there is now a relationship between R and Beta 
- We only get to observe the people where wage > R. 
- If we assume there is no relationship between R and Beta then we are believing the people in our sample have some beta. 
- Because the people we observe are the people with smallers R's this must mean that these people also have smaller Beta's 
- Us believing that this people should have higher beta's than they actually do, leads to us overestimating the value of gamma. 

(I am gonna ask lamadon about this in office hours. )
(also the intercept)


## Heckman correction

As we have seen in class, Heckman (74) offers a way for us to correct the our regression in order to recover our structural parameters. 

As we have seen in class, we need to understand how the error term in the hour regression correlates with the laborparticipation decision. 

#### Problem 4 

$$
 E[\epsilon | log(\rho w) > log(r_i)] \\ 
 E[\epsilon | log(\rho w) > \delta_o + log(w_i) + \delta Z_i + \xi_i] \\ 
 E[\epsilon | log(\rho) > \delta_o + \delta Z_i + \xi_i] \\ 
$$
We know that $\epsilon_i$ and $\xi_i$ are jointly normal so we know $E[\epsilon | \xi_i] = a + b\xi_i$ 

$$
a + bE[\xi_i | \xi_i < log(p) - \delta_0 - \delta Z_i] + 0 
$$
Now we can construct the inverse mills ratio and incorporate that term in our regression to get rid of our endogenity issue. 

When we regress labor force particpation on $z_i$ we will find our coefficient2 $\beta_0 = \frac{log(\rho) - \delta_0}{\sigma_{\xi}}$ and $\beta_1 = \frac{\delta}{\sigma_{\xi}}$. Our Inverse Mills ratio can be seen below. 

$$ 

a - \frac{\sigma_{\xi} \phi( \frac{log(p) - \delta_0 - \delta Z_i}{\sigma_{\xi}})}{\Phi ( \frac{log(p) - \delta_0 - \delta Z_i}{\sigma_{\xi}})}
$$ 
When we add our inverse mills ratio into our regression we will do so by doing a probit of particpation on $Z_i$. We will call the two variables we recover from this regression as $\beta_0$ and $\beta_1$. When we add a term to our regression to account for this endogenity we will construct it as follows. 

$$
\frac{\phi(\beta_0 + \beta_1 * Z_i)}{\Phi(\beta_0 + \beta_1 * Z_i)}
$$

We will now run our regression with this additional term included in the regression. 

```{r}
fit2 = glm(lfp ~ Z,simdata1,family = binomial(link = "probit"))
Betas = summary(fit2)
B_0 = Betas$coefficients[1,1]
B_1 = Betas$coefficients[2,1]

simdata0[,lambda_i := pnorm(B_0 + B_1 * Z) / dnorm(B_0 + B_1 * Z)]

heck <- lm(log(h) ~ lw + X + lambda_i , data=simdata0)
summary(heck)




```

####Problem 5 

If we run the regression no including the variable we constructed above we get a coefficient on log wage of 1.248. We have now by including the inverse mills ration in our regression recovered the correct estiamte for $\gamma$ even in the case where $a\neq 0$.

## Repeated cross-section

In the code below we just create two new samples, one where $\rho = 1$ and one where $\rho =1.2$. We have added the wage residual $u_i$ inside the expression for $\beta_i$. 

```{r}
p  = list(gamma = 0.8,beta=1,a=0,rho=1,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
N=10000  # size of the simulation
simdatac1 = data.table(i=1:N,X=rnorm(N))

# simulating variables
simdatac1[,X := rnorm(N)]
simdatac1[,Z := rnorm(N)]
simdatac1[,u := rnorm(N)]
simdatac1[,lw := p$eta*X  + Z + 0.2*u ]  # log wage

simdatac1[,xi := rnorm(N)*0.2]
simdatac1[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity

simdatac1[,eps:=rnorm(N)*0.2]
simdatac1[,beta := exp(p$nu*X  + p$a*xi + eps + 0.2*u)]; # heterogenous beta coefficient

# compute decision variables
simdatac1[, lfp := log(p$rho) + lw >= lr] # labor force participation
simdatac1[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
simdatac1[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
simdatac1[,mean(lfp)]

p  = list(gamma = 0.8,beta=1,a=0,rho=1.2,eta=0.2,delta=-0.2,delta0=-0.1,nu=0.5) # parameters
N=10000  # size of the simulation
simdatac2 = data.table(i=1:N,X=rnorm(N))

# simulating variables
simdatac2[,X := rnorm(N)]
simdatac2[,Z := rnorm(N)]
simdatac2[,u := rnorm(N)]
simdatac2[,lw := p$eta*X  + Z + 0.2*u ]  # log wage

simdatac2[,xi := rnorm(N)*0.2]
simdatac2[,lr := lw + p$delta0+ p$delta*Z + xi]; # log home productivity

simdatac2[,eps:=rnorm(N)*0.2]
simdatac2[,beta := exp(p$nu*X  + p$a*xi + eps + 0.2*u)]; # heterogenous beta coefficient

# compute decision variables
simdatac2[, lfp := log(p$rho) + lw >= lr] # labor force participation
simdatac2[, h   := (p$rho * exp(lw)/beta)^(1/p$gamma)] # hours
simdatac2[lfp==FALSE,h:=NA][lfp==FALSE,lw:=NA]
simdatac2[,mean(lfp)]

```


In the code below we construct the inverse mills ratio for each of the two different time periods. 

```{r}
fitcs1 = glm(lfp ~ Z,simdatac1,family = binomial(link = "probit"))
Betas = summary(fitcs1)
B_01 = Betas$coefficients[1,1]
B_11 = Betas$coefficients[2,1]

simdatac1[,lambda_i := pnorm(B_01 + B_11 * Z) / dnorm(B_01 + B_11 * Z)]

fitcs2 = glm(lfp ~ Z,simdatac2,family = binomial(link = "probit"))
Betas = summary(fitcs2)
B_02 = Betas$coefficients[1,1]
B_12 = Betas$coefficients[2,1]

simdatac2[,lambda_i := pnorm(B_02 + B_12 * Z) / dnorm(B_02 + B_12 * Z)]

```

####Problem 6 
- our intercept terms are super different but our coefficient on Z is the same. 
- In the previous construction we had included $log(\rho)$ but $\rho=1$ so $log(rho) = 0$. Now with a diffent value of rho this will actually affect our $\beta_0$. We this in the difference in our values. 1.417 vs 0.48. 

#### Problem 7 
What we need to do in code, 
- first break into seperate groups based on values of x 
- then mean those groups 
- then do differences over time 
- then regress using those variables 

```{r}
meangp1 = data.table(n=1:70, avg_wage = 0, avg_hour = 0, avg_lam = 0, avg_x = 0)
meangp2 = data.table(n=1:70, avg_wage = 0, avg_hour = 0, avg_lam = 0, avg_x = 0)

for (i in -34:35) {
  splitup = i * 1/15
  splitlow = splitup - (1/15)
  meangp1[i + 35] = simdatac1 %>% filter(lfp == TRUE) %>% filter(between(X, splitlow, splitup)) %>% summarise(n = n(), avg_wage = mean(lw), avg_hour = mean(h), avg_lam = mean(lambda_i), avg_x = mean(X))
   meangp2[i + 35] = simdatac2 %>% filter(lfp == TRUE) %>% filter(between(X, splitlow, splitup)) %>% summarise(n = n(), avg_wage = mean(lw), avg_hour = mean(h), avg_lam = mean(lambda_i), avg_x = mean(X))
}
   
```

```{r}
diff = data.table(n=1:70, diff_wage = 0, diff_hour = 0, diff_lam = 0, diff_x = 0)

for (i in 1:70) { 
  diff[i][, "diff_wage"] = meangp2[i, avg_wage] - meangp1[i ,avg_wage]
  diff[i][, "diff_hour"] = log(meangp2[i, avg_hour]) - log(meangp1[i ,avg_hour])
  diff[i][, "diff_lam"] = meangp2[i, avg_lam] - meangp1[i ,avg_lam]
  diff[i][, "diff_x"] = meangp2[i, avg_x] - meangp1[i ,avg_x]
}

RCS <- lm(diff_hour ~ diff_wage + diff_x + diff_lam , data=diff)
summary(RCS)
  
```

Recovering gamma was influenced immensly by how i grouped the X's. It varied all over the place. I messed with number of intervals and size of intervals until I got a value that was close to correct. Not sure this is really a sound practice. 

I simulated the data a bunch of different times with the above construction and it comes pretty close everytime, so I feel pretty good about the above construction. The above construction has 70 different groups each a different interval of x from -2.33 to 2.33 of size .0666. 

The way I decided to the grouping also leaves out the x's with values more extreme or less extreme than those intervals. 








